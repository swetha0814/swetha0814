{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "bd30d795-6c65-4dd8-8bab-9d315a535d69",
      "cell_type": "code",
      "source": "import numpy as np\nimport random\n\n# Define environment parameters\nNUM_STATES = 10  # Simplified state space (e.g., vehicle queue level: 0-9)\nNUM_ACTIONS = 2  # 0 = NS Green, 1 = EW Green\nEPISODES = 5000\nALPHA = 0.1      # Learning rate\nGAMMA = 0.9      # Discount factor\nEPSILON = 0.1    # Exploration rate\n\n# Initialize Q-table\nQ = np.zeros((NUM_STATES, NUM_ACTIONS))\n\n# Simulated traffic environment\ndef get_next_state(state, action):\n    if action == 0:\n        # NS green - reduce NS queue, increase EW\n        ns = max(0, state - random.randint(1, 3))\n        ew = min(NUM_STATES - 1, random.randint(0, 3))\n    else:\n        # EW green - reduce EW queue, increase NS\n        ew = max(0, state - random.randint(1, 3))\n        ns = min(NUM_STATES - 1, random.randint(0, 3))\n    return (ns + ew) // 2  # simplified next state (average queue level)\n\ndef get_reward(state, action):\n    # Lower state value means shorter queue, which is better\n    return -state  # reward is negative queue length\n\n# Q-learning algorithm\nfor episode in range(EPISODES):\n    state = random.randint(0, NUM_STATES - 1)\n    for t in range(100):  # time steps per episode\n        # Choose action (epsilon-greedy)\n        if random.random() < EPSILON:\n            action = random.randint(0, NUM_ACTIONS - 1)\n        else:\n            action = np.argmax(Q[state])\n\n        next_state = get_next_state(state, action)\n        reward = get_reward(state, action)\n\n        # Update Q-value\n        Q[state][action] += ALPHA * (reward + GAMMA * np.max(Q[next_state]) - Q[state][action])\n        state = next_state\n\n# Test policy\nprint(\"Trained Q-table:\")\nprint(Q)\n\ndef get_optimal_action(queue_level):\n    return \"NS Green\" if np.argmax(Q[queue_level]) == 0 else \"EW Green\"\n\n# Example decision\ncurrent_queue = 6\nprint(f\"Current queue level: {current_queue}, Suggested signal: {get_optimal_action(current_queue)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Trained Q-table:\n[[ -4.03164644  -4.43169669]\n [ -5.4889901   -5.2607923 ]\n [ -6.47786239  -6.58700204]\n [ -7.80418585  -7.85676884]\n [ -9.26439613  -9.48143601]\n [-11.12450764 -11.01692507]\n [-11.93745828 -12.85196387]\n [-14.26812176 -13.91514942]\n [-16.31077629 -16.05713631]\n [-17.707327   -17.3981601 ]]\nCurrent queue level: 6, Suggested signal: NS Green\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "df12f8b3-8f13-438f-9435-d86e3be2a7a1",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}